import json
import uuid
import os

INPUT_FILE = "AH_Stock_V6_Cloud.json"
OUTPUT_FILE = "AH_Stock_V7.3_FullStack.json"
DEMO_API_URL = "http://replace-with-your-cloud-run-url.com"

# --- V7 Node Definitions ---

def create_full_analysis_node():
    return {
        "parameters": {
            "url": f"{DEMO_API_URL}/analyze_full",
            "method": "POST",
            "authentication": "none",
            "sendBody": True,
            "contentType": "json",
            "bodyParameters": {
                "parameters": [
                    {"name": "code", "value": "={{ $json.code }}"},
                    {"name": "balance", "value": 100000},
                    {"name": "risk", "value": 0.01}
                ]
            },
            "options": {}
        },
        "id": str(uuid.uuid4()),
        "name": "Full Stack Analysis (API)",
        "type": "n8n-nodes-base.httpRequest",
        "typeVersion": 4.1,
        "position": [-1850, 450],
        "retryOnFail": True,
        "maxTries": 3,
        "waitBetweenTries": 3000,
        "onError": "continueRegularOutput" # CRITICAL: Prevent workflow stop on API error
    }

def update_ai_prompt_code(node):
    new_code = r"""
const data = $json;
// Error Handling: If API failed, provide fallback data
if (data.error) {
    return { json: { prompt: "Error in analysis: " + JSON.stringify(data), raw_data: data } };
}

const tech = data.technical || {};
const sig = data.signal || {};
const risk = data.risk_ctrl || {};
const marketStatus = $('Global Context (API)').item.json.market_status || "Unknown";

// èŽ·å–æ–°é—»æ•°æ®
let news_text = "æœªæœç´¢åˆ°æ–°é—»";
try {
    const news = $('Tavilyæ–°é—»æœç´¢').item.json;
    if (news.results) {
        news_text = news.results.map(r => `- ${r.title}`).join('\n');
    } else if (news.answer) news_text = news.answer;
} catch(e) {}

const prompt = `# å†³ç­–ä»ªè¡¨ç›˜åˆ†æžè¯·æ±‚

## è‚¡ç¥¨åŸºç¡€
- ä»£ç : ${data.code} (${data.market})
- å¤§ç›˜çŽ¯å¢ƒ: ${marketStatus}
- çŽ°ä»·: ${tech.current_price}

## ä¿¡å·ç³»ç»Ÿ (V7.1 Hybrid)
- æ ¸å¿ƒä¿¡å·: ${sig.signal}
- è¶‹åŠ¿è¯„åˆ†: ${sig.trend_score}/100
- ç†ç”±: ${sig.signal_reasons ? sig.signal_reasons.join(', ') : 'æ— '}

## é£ŽæŽ§å‚æ•°
- æ­¢æŸä»·: ${sig.stop_loss}
- ç›®æ ‡ä»·: ${sig.take_profit}
- ATRæ³¢åŠ¨: ${tech.atr14}
- å»ºè®®ä»“ä½: ${risk.suggested_position} æ‰‹ (åŸºäºŽ1%é£Žé™©)

## æŠ€æœ¯æŒ‡æ ‡
- MAæŽ’åˆ—: ${tech.ma_alignment} (${tech.ma5}/${tech.ma20})
- EMAè¶‹åŠ¿: 13æ—¥çº¿ ${tech.ema13} vs 26æ—¥çº¿ ${tech.ema26}
- ä¹–ç¦»çŽ‡: ${tech.bias_ma5}%
- RSI(14): ${tech.rsi14}
- é‡æ¯”: ${tech.volume_ratio}

## æ–°é—»æ‘˜è¦
${news_text}

è¯·åŸºäºŽä»¥ä¸Šæ•°æ®ï¼Œç”¨ç®€ç»ƒçš„ä¸“ä¸šæœ¯è¯­ç”Ÿæˆäº¤æ˜“è®¡åˆ’ã€‚
`;

// --- åŠ¨æ€é£Žé™©æç¤ºé€»è¾‘ ---
let risk_instruction = "";
if (marketStatus === 'Bear' || marketStatus === 'Crash') {
    risk_instruction = `
!!! ðŸ”´ ä¸¥é‡è­¦å‘Šï¼šå½“å‰å¤„äºŽç†Šå¸‚/æš´è·ŒçŽ¯å¢ƒ (${marketStatus}) !!!
1. **ä¸¥æ ¼æ ‡å‡†**ï¼šä»…å…è®¸è¯„åˆ†>85ä¸”æœ‰é‡å¤§åˆ©å¥½çš„ä¸ªè‚¡æ“ä½œã€‚
2. **å¼ºåˆ¶è½»ä»“**ï¼šå»ºè®®ä»“ä½å¿…é¡»å‡åŠï¼Œæˆ–è€…ç›´æŽ¥å»ºè®®ç©ºä»“ã€‚
3. **å¯»æ‰¾åšç©ºæœºä¼š**ï¼šå¦‚æžœæ˜¯æ¸¯è‚¡ï¼Œä¼˜å…ˆå¯»æ‰¾åšç©ºé€»è¾‘ï¼›å¦‚æžœæ˜¯Aè‚¡ï¼Œå»ºè®®ä»¥â€œè§‚æœ›â€ä¸ºä¸»ã€‚
4. **æŽªè¾žä¸¥åŽ‰**ï¼šè¯·åœ¨æ ¸å¿ƒç»“è®ºä¸­ç”¨ç²—ä½“å¼ºè°ƒå¤§ç›˜é£Žé™©ã€‚`;
}

return {
    json: {
        prompt: prompt + risk_instruction,
        raw_data: data
    }
};
"""
    node['parameters']['jsCode'] = new_code
    return node

def update_feishu_write(node):
    # Update mappings for new fields
    v7_body = r'''={{
JSON.stringify({
  "records": [
    {
      "fields": {
        "æ—¥æœŸ": new Date().getTime(),
        "ä»£ç ": $('Full Stack Analysis (API)').item.json.code,
        "åç§°": $('Full Stack Analysis (API)').item.json.name,
        "å¤§ç›˜çŠ¶æ€": $('Global Context (API)').item.json.market_status,
        
        "ä¿¡å·ç±»åž‹": $('Full Stack Analysis (API)').item.json.signal ? $('Full Stack Analysis (API)').item.json.signal.signal : "Error",
        "æ“ä½œå»ºè®®": $json.operation_advice, 
        "æ ¸å¿ƒç»“è®º": $json.one_sentence,
        
        "å»ºè®®ä»“ä½(æ‰‹)": $('Full Stack Analysis (API)').item.json.risk_ctrl ? $('Full Stack Analysis (API)').item.json.risk_ctrl.suggested_position : 0,
        "æ­¢æŸä»·": $('Full Stack Analysis (API)').item.json.signal ? $('Full Stack Analysis (API)').item.json.signal.stop_loss : 0,
        "ATR": $('Full Stack Analysis (API)').item.json.technical ? $('Full Stack Analysis (API)').item.json.technical.atr14 : 0,
        "RSI": $('Full Stack Analysis (API)').item.json.technical ? $('Full Stack Analysis (API)').item.json.technical.rsi14 : 0,
        "é‡æ¯”": $('Full Stack Analysis (API)').item.json.technical ? $('Full Stack Analysis (API)').item.json.technical.volume_ratio : 0,
        "å‡çº¿å½¢æ€": $('Full Stack Analysis (API)').item.json.technical ? $('Full Stack Analysis (API)').item.json.technical.ma_alignment : "",
        
        "é£Žé™©è­¦æŠ¥": $json.risk_alerts,
        "æ£€æŸ¥æ¸…å•": $json.checklist
      }
    }
  ]
})
}}'''
    node['parameters']['body'] = v7_body
    return node

# === Build Logic ===

if not os.path.exists(INPUT_FILE):
    print(f"Error: {INPUT_FILE} not found.")
    exit(1)

with open(INPUT_FILE, 'r', encoding='utf-8') as f:
    workflow = json.load(f)

nodes = workflow.get('nodes', [])
connections = workflow.get('connections', {})

# 1. Removal List (Include all legacy + Filter + Merge)
nodes_to_remove = [
    "è®¡ç®—Aè‚¡æŠ€æœ¯æŒ‡æ ‡", "è®¡ç®—æ¸¯è‚¡æŠ€æœ¯æŒ‡æ ‡", "åˆå¹¶æŠ€æœ¯æŒ‡æ ‡", 
    "Stock Risk Analysis (API)", "Stock Risk Analysis",
    "Merge Analysis Data",
    "åˆ¤æ–­å¸‚åœºç±»åž‹", "è…¾è®¯è´¢ç»_å®žæ—¶è¡Œæƒ…", "YahooFinance_æ¸¯è‚¡æ•°æ®",
    "è§£æžè…¾è®¯æ•°æ®", "è§£æžYahooæ•°æ®", "è…¾è®¯è´¢ç»_Kçº¿æ•°æ®",
    "Market Regime Filter", # Removed for Bear Mode
    "åˆå¹¶æŽ¨é€ç»“æžœ" # Removed for Loop Fix
]

workflow['nodes'] = [n for n in nodes if n['name'] not in nodes_to_remove]
nodes = workflow['nodes']

# 2. Add New Nodes
analysis_node = create_full_analysis_node()
workflow['nodes'].append(analysis_node)

# 3. Locate Key Nodes
trigger = next(n for n in nodes if n['name'] == 'å®šæ—¶è§¦å‘_å·¥ä½œæ—¥18ç‚¹')
global_ctx = next(n for n in nodes if n['name'] == 'Global Context (API)')
read_list = next(n for n in nodes if n['name'] == 'è¯»å–è‚¡ç¥¨åˆ—è¡¨')
split_node = next(n for n in nodes if n['name'] == 'å¾ªçŽ¯å¤„ç†æ¯åªè‚¡ç¥¨')
tavily = next(n for n in nodes if n['name'] == 'Tavilyæ–°é—»æœç´¢')
prompt_node = next(n for n in nodes if n['name'] == 'æž„å»ºAIæç¤ºè¯')
feishu_write = next(n for n in nodes if n['name'] == 'å†™å…¥é£žä¹¦')
feishu_buy = next(n for n in nodes if n['name'] == 'é£žä¹¦å‘é€æ¶ˆæ¯_ä¹°å…¥')
feishu_other = next(n for n in nodes if n['name'] == 'é£žä¹¦å‘é€æ¶ˆæ¯_å…¶ä»–')

# 4. Updates
prompt_node = update_ai_prompt_code(prompt_node)
feishu_write = update_feishu_write(feishu_write)

# 5. Settings Update (Batch Size)
if 'parameters' not in split_node: split_node['parameters'] = {}
if 'options' not in split_node['parameters']: split_node['parameters']['options'] = {}
split_node['parameters']['options']['reset'] = False
split_node['parameters']['batchSize'] = 1

# 6. Wiring (The Clean Sequence)

conns = {} # Rebuild specific connections to be safe

# Trigger -> Global Context
conns[trigger['name']] = {"main": [[{"node": global_ctx['name'], "type": "main", "index": 0}]]}

# Global Context -> Read List (Direct)
conns[global_ctx['name']] = {"main": [[{"node": read_list['name'], "type": "main", "index": 0}]]}

# Read List -> SplitBatch
conns[read_list['name']] = {"main": [[{"node": split_node['name'], "type": "main", "index": 0}]]}

# SplitBatch -> Full Analysis
conns[split_node['name']] = {"main": [
    [{"node": analysis_node['name'], "type": "main", "index": 0}], # Loop
    [] # Done
]}

# Full Analysis -> Tavily
conns[analysis_node['name']] = {"main": [[{"node": tavily['name'], "type": "main", "index": 0}]]}

# Tavily -> Prompt
conns[tavily['name']] = {"main": [[{"node": prompt_node['name'], "type": "main", "index": 0}]]}

# Prompt -> AI Agent
ai_agent = next(n for n in nodes if n['name'] == 'AIåˆ†æžAgent')
conns[prompt_node['name']] = {"main": [[{"node": ai_agent['name'], "type": "main", "index": 0}]]}

# AI Agent -> Parse (Existing in JSON)
parse_node = next(n for n in nodes if n['name'] == 'è§£æžAIåˆ†æžç»“æžœ')
conns[ai_agent['name']] = {"main": [[{"node": parse_node['name'], "type": "main", "index": 0}]]}

# Parse -> Feishu Write
conns[parse_node['name']] = {"main": [[{"node": feishu_write['name'], "type": "main", "index": 0}]]}

# Feishu Write -> Decision (Existing)
decision_node = next(n for n in nodes if n['name'] == 'åˆ¤æ–­æ˜¯å¦ä¹°å…¥ä¿¡å·')
conns[feishu_write['name']] = {"main": [[{"node": decision_node['name'], "type": "main", "index": 0}]]}

# Decision -> Build Cards (Existing)
buy_card = next(n for n in nodes if n['name'] == 'æž„å»ºé£žä¹¦å¡ç‰‡_ä¹°å…¥')
other_card = next(n for n in nodes if n['name'] == 'æž„å»ºé£žä¹¦å¡ç‰‡_å…¶ä»–')
conns[decision_node['name']] = {"main": [
    [{"node": buy_card['name'], "type": "main", "index": 0}],
    [{"node": other_card['name'], "type": "main", "index": 0}]
]}

# Build Cards -> Send Messages (Existing)
conns[buy_card['name']] = {"main": [[{"node": feishu_buy['name'], "type": "main", "index": 0}]]}
conns[other_card['name']] = {"main": [[{"node": feishu_other['name'], "type": "main", "index": 0}]]}

# Send Messages -> SplitBatch (CLOSING THE LOOP DIRECTLY)
# This is the fix for the deadlock
conns[feishu_buy['name']] = {"main": [[{"node": split_node['name'], "type": "main", "index": 0}]]}
conns[feishu_other['name']] = {"main": [[{"node": split_node['name'], "type": "main", "index": 0}]]}

# Preserve Gemini Model Connection
gemini_model = next((n for n in nodes if n['name'] == 'Google Gemini Chat Model'), None)
if gemini_model:
    conns[gemini_model['name']] = connections.get(gemini_model['name'], {})

workflow['connections'] = conns

with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
    json.dump(workflow, f, indent=2, ensure_ascii=False)

print(f"Success! Generated {OUTPUT_FILE} with loop fixes and error handling.")
